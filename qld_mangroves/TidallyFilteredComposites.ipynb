{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidally Filtered Satellite Composites\n",
    "\n",
    "### Background\n",
    "The intertidal zones across Australia possess significant environmental value, and provide a host of ecosystem services that are often poorly valued. However, mapping the extent of the major intertidal land classes (e.g. mangroves, salt marsh, clay pans) is a difficult challenge given the vast extent of Australia's coastline.  The Sentinel 2 archive in Digital Earth Australia's datacube provides a means to map the intertidal zone at scale by providing complete satellite coverage of Australia at 10m spatial resolution and a 3-5 day return time. \n",
    "\n",
    "### Description\n",
    "For mapping the extent of mangroves, salt marsh, and clay-pans in the intertidal zone we need annual composites of the mid-tide conditions to reduce noise from water inundation. This notebook will load in Sentinel 2 data from the datacube, calculate the tide height at the time of each satellite observation, and then create an image composite of only those satellite images that are within a specified tidal range. The code will also produce a number of vegetation indice summary statistics (the `Modified Soil Adjusted Vegetation Index, MSAVI`: max, min, std dev, and range), along with a map of indundation frequency, to assist in the classification script `RandomForest_classifier.ipynb`.  The workflow is as follows:  \n",
    "\n",
    "1.  Load in a Sentinel 2 time-series, and cloud mask using a prototype time-series based cloud mask.\n",
    "2.  Calculate the tide height for each satellite image. \n",
    "3.  Filter the images by the tidal range that encompasses the mid-tide height.\n",
    "4.  Calculate summary phenology statistics on `MSAVI`, and calculate the mean `Modified, Normalised Difference Water Index (MNDWI)`.\n",
    "5.  Calculate normalised inundation frequency using MNDWI.\n",
    "6.  Create annual geomedian composite images of the spectral bands. \n",
    "7.  Join the phenology summary stats and the spectral geomedians together and export the results as a netcdf file.\n",
    "8.  Export a true and false colour geotiff to assist with creating a training dataset.\n",
    "\n",
    "### Technical details\n",
    "\n",
    "* Products used: `s2a_ard_granule`, `s2b_ard_granule`\n",
    "* Analyses used: `tide modelling`, `image compositing: Geomedians` `MSAVI` `MNDWI`\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before this notebook can run on the Sandbox, the `ClassificationTools.py` and `SpatialTools.py` scripts need to be placed in the _Case_Studies/utils_ folder:\n",
    " \n",
    "You also need to install a few libraries that aren't by default installed on the DEA Sandbox.  Run the cell below to load in the libraries (you only need to do this once, thereafter the libraries will be available). Once you've successfully run the installation, you will need to restart the kernel before the libraries will work.  Click `kernel--> Restart kernel and clear all outputs`.\n",
    "\n",
    "Delete the cell below once you've successfully installed the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install non-default libaraies\n",
    "# !pip install --user git+https://github.com/opendatacube/datacube-stats/\n",
    "# !pip install --user hdmedians\n",
    "# !pip install --user spectral\n",
    "# !pip install --user scikit-learn\n",
    "\n",
    "#install the prototype cloud masking algorithm\n",
    "# !git clone https://github.com/GeoscienceAustralia/dea-tsmask.git\n",
    "# !python3 dea-tsmask/setup.py build\n",
    "# !python3 dea-tsmask/setup.py install --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Once you have successful installed the non-default libraries, run this analysis by running all the cells in the notebook (shift+enter will run a highlighted cell), starting with the \"Load modules\" cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import datacube_stats\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from datacube.virtual import catalog_from_file, construct_from_yaml\n",
    "from datacube.virtual import Transformation, DEFAULT_RESOLVER\n",
    "from datacube.utils.dask import start_local_dask\n",
    "import dask\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from dea_tsmask import tsmask_temporal\n",
    "from datacube.helpers import write_geotiff\n",
    "import warnings\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import DEA Notebooks scripts\n",
    "sys.path.append('../Scripts')\n",
    "import dea_datahandling\n",
    "import dea_coastaltools\n",
    "from dea_plotting import display_map\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#This will speed up loading data\n",
    "datacube.utils.rio.set_default_rio_config(aws='auto', cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"distributed.dashboard.link\": \n",
    "        os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"})\n",
    "\n",
    "client = start_local_dask(n_workers=4, threads_per_worker=1, memory_limit='6G')\n",
    "creds = configure_s3_access(client=client, \n",
    "                            region_name='auto')\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs to set up analysis\n",
    "\n",
    "The following lines allow us to set up our analysis by selecting the location (`lat_range` and `lon_range`) and time (`time_range`) we want to investigate.\n",
    "\n",
    "* **lat:** Latitude of the central point in your AOI (e.g. `-12.20`).\n",
    "\n",
    "* **lon:** Longitude of the central point in your AOI (e.g. `131.80`). \n",
    "\n",
    "* **buffer:** The distance (in decimal degrees) around your central lat/lon point. For fast load times, keep this < 0.04 degrees.\n",
    "\n",
    "* **time_range:** Enter a year to collect data from (just do one year at a time e.g. `'2018'` )\n",
    "\n",
    "* **tide_lat/lon:** The tidal model used in this analysis can only model tide heights correctly if the centre of your study area is located over open ocean. To avoid having the model fail, specify a tide modelling location that is over the open ocean nearest to the AOI you want to investigate. \n",
    "\n",
    "* **tide_range_buffer:** A buffer value (in metres, e.g `0.3`) is required to stipulate what range of tidal values should be collected to allow for image compositing of the mid-tide conditions (buffer value is centred around 0m, ie.e Mean Sea Level at that location). A greater number will include more images in the image compositing algorithm, but at the cost of intergrating greater variation of tidal conditions into the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit these lines to change any of the analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = -27.651  \n",
    "lon = 153.33 \n",
    "\n",
    "buffer = 0.02\n",
    "\n",
    "time_range = '2018'\n",
    "\n",
    "tide_lat = -27.651\n",
    "tide_lon = 153.33\n",
    "\n",
    "tide_range_buffer = 0.3\n",
    "\n",
    "#set a location to put the results\n",
    "results = 'results/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the selected location\n",
    "\n",
    "The next cell will display the selected area on an interactive map. Feel free to zoom in and out to get a better understanding of the area you'll be analysing. Clicking on any point of the map will reveal the latitude and longitude coordinates of that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_map(y=(lat-buffer, lat + buffer), x=(lon-buffer, lon + buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cloud-masked Sentinel 2 data\n",
    "The first step in this analysis is to load in cloud-masked Sentinel 2 data for the `lat_range`, `lon_range` and `time_range` we provided above. We will be doing this using a protoype time-series based cloud masking approach (the algorithm looks for outliers) being developed by Peter Tan at GA. This method is slow but will likely work better for our purposes. For the algorithm to work successfully, it requires at least 90 satellite images, so loading in < a year's worth of images is not advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object, which contains the longitudes, latitudes and time provided above\n",
    "dc = datacube.Datacube(app='tidallyfilteredcomposites')\n",
    "\n",
    "query = {\n",
    "    'y':(lat-buffer, lat+buffer),\n",
    "    'x': (lon-buffer, lon+buffer),\n",
    "    'time': (time_range),\n",
    "    'dask_chunks' : {'x':200, 'y':200},\n",
    "    'output_crs': 'EPSG:3577',\n",
    "    'resolution': (-10, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series cloud mask\n",
    "\n",
    "This is a prototype cloud mask that tends to work well in coastal regions. The code below will take a long time to run, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Class and functions below assist with creating the time-series mask\n",
    "\n",
    "class TSMask(Transformation):\n",
    "    def compute(self, data):\n",
    "        chunks = dict(time=-1)\n",
    "        result = xr.apply_ufunc(tsmask_temporal,\n",
    "                                data.avg.chunk(chunks), data.mndwi.chunk(\n",
    "                                    chunks), data.msavi.chunk(chunks),\n",
    "                                dask='parallelized', output_dtypes=['uint8']).assign_attrs(nodata=0, units='1')\n",
    "        return result.to_dataset(name='classification').assign_attrs(**data.attrs)\n",
    "\n",
    "    def measurements(self, input_measurements):\n",
    "        return {'classification': Measurement(name='classification', dtype='uint8', nodata=0, units='1')}\n",
    "\n",
    "def load_catalog():\n",
    "    resolver = DEFAULT_RESOLVER.clone()\n",
    "    resolver.register('transform', 'tsmask', TSMask)\n",
    "    return catalog_from_file('../../../dea-tsmask/virtual-product-catalog.yaml',\n",
    "                             name_resolver=resolver)\n",
    "\n",
    "catalog = load_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and cloud mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will calculate the masvi, and mndwi indices\n",
    "print('calculating indices')\n",
    "s2_indices = catalog['s2_indices'].load(dc, **query)\n",
    "\n",
    "# this will compute the ts cloud mask\n",
    "print('calculating cloud_mask')\n",
    "cloud_mask = catalog['s2_tsmask'].load(dc, **query)\n",
    "# cloud_mask = cloud_mask.compute()\n",
    "\n",
    "# load the raw, unmasked Sentinel data (only the bands we're interested in)\n",
    "bands = ['nbart_blue','nbart_green',\n",
    "         'nbart_red','nbart_red_edge_1',\n",
    "         'nbart_red_edge_2','nbart_red_edge_3',\n",
    "         'nbart_nir_1','nbart_nir_2',\n",
    "         'nbart_swir_2','nbart_swir_3']\n",
    "\n",
    "query['measurements'] = bands\n",
    "sentinel_ds = dea_datahandling.load_ard(dc=dc,\n",
    "                                        lazy_load=True,\n",
    "                                        mask_pixel_quality=False,\n",
    "                                        mask_invalid_data=False,\n",
    "                                        products=['s2a_ard_granule',\n",
    "                                                  's2b_ard_granule'],\n",
    "                                        **query)\n",
    "\n",
    "# mask the sentinel data with the cloud mask\n",
    "sentinel_ds = sentinel_ds.where(cloud_mask.classification==1)\n",
    "s2_indices_clear =  s2_indices.where(cloud_mask.classification==1)\n",
    "\n",
    "# merge the indices with the nbart data\n",
    "sentinel_ds['mndwi'] = s2_indices_clear.mndwi\n",
    "sentinel_ds['msavi'] = s2_indices_clear.msavi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tide heights\n",
    "As mangroves are in the inter-tidal zone, we aim to reduce the effect of tides by first modelling the tide height, and then keeping only the satellite images that were taken at the mid-tide conditions. For example, if `tide_range_buff = 0.3`, we are telling the analysis to focus only on satellite images taken when the tide was between `-0.30 m` and `+0.30 m` of the MSL.\n",
    "\n",
    "The `tidal_tag` function below uses the [OTPS TPXO9 tidal model](http://volkov.oce.orst.edu/tides/global.html) to calculate the height of the tide at the exact moment each satellite image in our dataset was taken, and adds this as a new `tide_height` attribute in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_ds = dea_coastaltools.tidal_tag(\n",
    "    ds=sentinel_ds,\n",
    "    tidepost_lat=tide_lat,\n",
    "    tidepost_lon=tide_lon\n",
    ")\n",
    "sentinel_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have modelled tide heights, we can plot them to visualise the range of tide that was captured by Sentinel across our time series. In the plot below, red dashed lines also show the subset of the tidal range we selected using the `tide_range_buffer` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting tide heights for each sentinel image:\n",
    "tide_range = (0-tide_range_buffer, 0+tide_range_buffer)\n",
    "\n",
    "sentinel_ds.tide_height.plot()\n",
    "plt.axhline(0, c='black', linestyle='--')\n",
    "plt.axhline(tide_range[0], c='red', linestyle='--')\n",
    "plt.axhline(tide_range[1], c='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Sentinel images by tide height\n",
    "Here we take the Sentinel 2 dataset and only keep the images within the tide heights given by `tide_range_buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_filtered_all = sentinel_ds.where((sentinel_ds.tide_height > tide_range[0]) & \n",
    "                                    (sentinel_ds.tide_height < tide_range[1]), drop=True)\n",
    "\n",
    "print(\"We retained \"+ str(len(sentinel_filtered_all.time.values)) + \" images from within the nominated mid-tide range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_filtered_all = sentinel_filtered_all.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate summary phenology statistics\n",
    "\n",
    "These will help in our random forest classifier with distinguishing the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vegetation indice summary statistics\n",
    "msavi_mean = sentinel_filtered_all.msavi.mean('time').rename('msavi_mean')\n",
    "msavi_std = sentinel_filtered_all.msavi.std('time').rename('msavi_std')\n",
    "msavi_min = sentinel_filtered_all.msavi.min('time').rename('msavi_min')\n",
    "msavi_max = sentinel_filtered_all.msavi.max('time').rename('msavi_max')\n",
    "msavi_range = msavi_max - msavi_min\n",
    "msavi_range = msavi_range.rename('msavi_range')\n",
    "\n",
    "#and seperate out a mean of our water index\n",
    "mndwi_mean = sentinel_filtered_all.mndwi.max('time').rename('mndwi_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate inundation frequency\n",
    "\n",
    "This will calculate the frequency with which each pixel is inundated with water (inundation is defined as times when MNDWI indice is > 0).\n",
    "\n",
    "Frequency is calculated as the number of times a pixel is observed as water divided by the number of valid observations of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where MNDWI is > 0 (is water), convert to boolean\n",
    "water = sentinel_ds.mndwi > 0\n",
    "#count the number of times we observe water per pixel\n",
    "count = water.sum(dim=['time'])\n",
    "#count number of valid observations per pixel\n",
    "valid_ds = np.isfinite(sentinel_ds.nbart_red)\n",
    "valid_sum = valid_ds.sum(dim=['time'])\n",
    "#calcuate normalised inundation frequency\n",
    "frequency = (count / valid_sum)\n",
    "frequency = frequency.rename('inundation_freq')\n",
    "frequency = frequency.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine observations into noise-free summary images\n",
    "Individual remote sensing images can be affected by noisy data, including clouds, sunglint and poor water quality conditions (e.g. sediment). To produce cleaner images that can be compared more easily across time, we can create 'summary' images or composites that combine multiple images into one image to reveal the median or 'typical' appearance of the landscape for a certain time period.  In the code below, we take the tidally filtered set of images and calculate an annual [Geomedian](https://github.com/daleroberts/hdmedians). A geomedian calculates a high-dimensional median value for each pixel, for each spectral band. In contrast to a standard median, a geomedian maintains the relationship between spectral bands. This allows us to conduct further analysis on the composite images just as we would on the original satellite images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoMedian composite\n",
    "\n",
    "This will take a couple of minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute annual geomedians from the entire mid-tide range for only the spectral bands\n",
    "sentinel_geomedian = GeoMedian().compute(sentinel_filtered_all.drop(['tide_height', 'msavi', 'mndwi']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine our phenology stats with geomedians into a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSummaryStats = xr.merge([sentinel_geomedian,msavi_mean,msavi_std,\n",
    "                            msavi_min,msavi_max,msavi_range,mndwi_mean,frequency])\n",
    "\n",
    "allSummaryStats.attrs = sentinel_ds.attrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSummaryStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export our results as a netcdf\n",
    "\n",
    "This file will be our input into the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube.drivers.netcdf.write_dataset_to_netcdf(allSummaryStats, results + 'allSummaryStats.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a true and false colour geotiff to assist with creating a training dataset\n",
    "\n",
    "When creating a training dataset, it's important to use the same imagery that will be used in classifier, rather than relying on image composites provided in google maps or esri basemaps as these will be from different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select just the rgb bands for the true colour geotiff\n",
    "rgb = xr.merge([sentinel_geomedian.nbart_red, sentinel_geomedian.nbart_green, sentinel_geomedian.nbart_blue])\n",
    "rgb.attrs = sentinel_geomedian.attrs\n",
    "\n",
    "#select the swir, nir, and green bands for the false colour image\n",
    "sng = xr.merge([sentinel_geomedian.nbart_swir_2, sentinel_geomedian.nbart_nir_1, sentinel_geomedian.nbart_green])\n",
    "sng.attrs = sentinel_geomedian.attrs\n",
    "\n",
    "#write out the results\n",
    "write_geotiff(results+'true_colour.tif', rgb)\n",
    "write_geotiff(results+'false_colour.tif', sng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to check out the geomdian geotiffs look right\n",
    "b=['nbart_red','nbart_green', 'nbart_blue']\n",
    "da = rgb[b].to_array()\n",
    "img = da.plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### That completes this script, now open `RandomForest_classifier.ipynb` to use the tidally filtered composites we just created to classify the intertidal zone in you're AOI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
